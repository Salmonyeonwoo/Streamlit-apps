# ========================================
# streamlit_app_final.py
# ì™„ì„±ë³¸: Streamlit ì•± â€” Whisper ì „ì‚¬, Firestore/GCS í†µí•©, ì‹œë®¬ë ˆì´í„°, RAG, LSTM
# ========================================

import streamlit as st
import os
import tempfile
import time
import json
import re
import base64
import io
import numpy as np
from bs4 import BeautifulSoup
import matplotlib.pyplot as plt
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense
from datetime import datetime, timedelta, timezone 
from openai import OpenAI

# â­ Firebase / GCS
import firebase_admin
from firebase_admin import credentials, firestore, initialize_app, get_app
from google.cloud import storage
from google.cloud.exceptions import NotFound 
from google.cloud import firestore as gcp_firestore
from google.cloud.firestore import Query 

# LangChain Imports (ì‚¬ìš©í•˜ì§€ ì•ŠëŠ” ê²½ìš° ì£¼ì„ ì²˜ë¦¬ ê°€ëŠ¥)
from langchain.chains import ConversationalRetrievalChain, ConversationChain
from langchain_community.document_loaders import PyPDFLoader, TextLoader
from langchain_community.vectorstores import FAISS
from langchain_google_genai import ChatGoogleGenerativeAI, GoogleGenerativeAIEmbeddings
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.memory import ConversationBufferMemory
from langchain.schema.document import Document
from langchain.prompts import PromptTemplate 

# -----------------------------
# 1. Config & I18N (ë‹¤êµ­ì–´ ì§€ì›)
# -----------------------------
DEFAULT_LANG = "ko"
# st.session_state ì ‘ê·¼ì€ ë°˜ë“œì‹œ ì²« st.set_page_config ì´í›„ì— ì´ë£¨ì–´ì ¸ì•¼ í•©ë‹ˆë‹¤.
# ì´ˆê¸°í™” ì „ì—ëŠ” DEFAULT_LANGìœ¼ë¡œ ì„¤ì •í•©ë‹ˆë‹¤.

LANG = {
    # ... (LANG ë”•ì…”ë„ˆë¦¬ ë‚´ìš©ì€ ìœ ì§€) ...
    "ko": {
        "title": "ê°œì¸ ë§ì¶¤í˜• AI í•™ìŠµ ì½”ì¹˜ (ìŒì„± ë° DB í†µí•©)",
        "sidebar_title": "ğŸ“š AI Study Coach ì„¤ì •",
        # (ë‚˜ë¨¸ì§€ í‚¤ ìœ ì§€)
        "voice_rec_header": 'ìŒì„± ê¸°ë¡ & ê´€ë¦¬',
        "record_help": 'ë§ˆì´í¬ ë²„íŠ¼ì„ ëˆŒëŸ¬ ë…¹ìŒí•˜ê±°ë‚˜ íŒŒì¼ì„ ì—…ë¡œë“œí•˜ì„¸ìš”ã€‚',
        "gcs_missing": 'GCS ë²„í‚·ì´ ì„¤ì •ë˜ì–´ ìˆì§€ ì•ŠìŠµë‹ˆë‹¤. Secretsì— GCS_BUCKET_NAMEì„ ì¶”ê°€í•˜ì„¸ìš”.',
        "openai_missing": 'OpenAI API Keyê°€ ì—†ìŠµë‹ˆë‹¤. Secretsì— OPENAI_API_KEYë¥¼ ì„¤ì •í•˜ì„¸ìš”ã€‚',
        "delete_fail": "ì‚­ì œ ì‹¤íŒ¨",
        "save_history_fail": "âŒ ìƒë‹´ ì´ë ¥ ì €ì¥ ì‹¤íŒ¨",
        "delete_success": "âœ… ëª¨ë“  ìƒë‹´ ì´ë ¥ ì‚­ì œ ì™„ë£Œ!",
        "firestore_no_index": "ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ ê¸°ì¡´ RAG ì¸ë±ìŠ¤ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. íŒŒì¼ì„ ì—…ë¡œë“œí•˜ì—¬ ìƒˆë¡œ ë§Œë“œì„¸ìš”ã€‚", 
        "lang_select": "ì–¸ì–´ ì„ íƒ",
        "embed_fail": "ì„ë² ë”© ì‹¤íŒ¨: ë¬´ë£Œ í‹°ì–´ í•œë„ ì´ˆê³¼ ë˜ëŠ” ë„¤íŠ¸ì›Œí¬ ë¬¸ì œã€‚",
        "gcs_not_conf": 'GCS ë¯¸ì„¤ì • ë˜ëŠ” ì˜¤ë””ì˜¤ ì—†ìŒ',
        "gcs_playback_fail": 'ì˜¤ë””ì˜¤ ì¬ìƒ ì‹¤íŒ¨',
        "gcs_no_audio": 'ì˜¤ë””ì˜¤ íŒŒì¼ ì—†ìŒ (GCS ë¯¸ì„¤ì •)',
        "transcribing": 'ìŒì„± ì „ì‚¬ ì¤‘...',
        "playback": 'ë…¹ìŒ ì¬ìƒ',
        "retranscribe": 'ì¬ì „ì‚¬',
        "error": 'ì˜¤ë¥˜:',
        # ...
    },
    "en": {
        "title": "Personalized AI Study Coach (Voice & DB Integration)",
        "sidebar_title": "ğŸ“š AI Study Coach Settings",
        "lang_select": "Select Language",
        "voice_rec_header": 'Voice Record & Management',
        "record_help": 'Press the microphone button to record or upload a file.',
        "gcs_missing": 'GCS bucket is not configured. Add GCS_BUCKET_NAME to Secrets.',
        "openai_missing": 'OpenAI API Key is missing. Set OPENAI_API_KEY in Secrets.',
        "delete_fail": "Deletion failed",
        "save_history_fail": "âŒ Simulation history save failed",
        "delete_success": "âœ… Successfully deleted!", 
        "firestore_no_index": "Could not find existing RAG index in database. Please upload files and create a new one.", 
        "embed_fail": "Embedding failed: Free tier quota exceeded or network issue.",
        "gcs_not_conf": 'GCS not configured or audio not available',
        "gcs_playback_fail": 'Audio playback failed',
        "gcs_no_audio": 'No audio file (GCS not configured)',
        "transcribing": 'Transcribing voice...',
        "playback": 'Playback Recording',
        "retranscribe": 'Re-transcribe',
        "error": 'Error:',
        # ...
    },
    "ja": {
        "title": "ãƒ‘ãƒ¼ã‚½ãƒŠãƒ©ã‚¤ã‚ºAIå­¦ç¿’ã‚³ãƒ¼ãƒ (éŸ³å£°ãƒ»DBçµ±åˆ)",
        "sidebar_title": "ğŸ“š AIå­¦ç¿’ã‚³ãƒ¼ãƒè¨­å®š",
        "lang_select": "è¨€èªé¸æŠ",
        "voice_rec_header": 'éŸ³å£°è¨˜éŒ²ã¨ç®¡ç†',
        "record_help": 'ãƒã‚¤ã‚¯ãƒœã‚¿ãƒ³ã‚’æŠ¼ã—ã¦éŒ²éŸ³ã™ã‚‹ã‹ã€ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„ã€‚',
        "gcs_missing": 'GCSãƒã‚±ãƒƒãƒˆãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚Secretsã«GCS_BUCKET_NAMEã‚’è¿½åŠ ã—ã¦ãã ã•ã„ã€‚',
        "openai_missing": 'OpenAI APIã‚­ãƒ¼ãŒã‚ã‚Šã¾ã›ã‚“ã€‚Secretsã«OPENAI_API_KEYã‚’è¨­å®šã—ã¦ãã ã•ã„ã€‚',
        "delete_fail": "å‰Šé™¤å¤±æ•—",
        "save_history_fail": "âŒ å¯¾å¿œå±¥æ­´ã®ä¿å­˜ã«å¤±æ•—ã—ã¾ã—ãŸ",
        "delete_success": "âœ… å‰Šé™¤ãŒå®Œäº†ã•ã‚Œã¾ã—ãŸ!", 
        "firestore_no_index": "ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã§æ—¢å­˜ã®RAGã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦æ–°ã—ãä½œæˆã—ã¦ãã ã•ã„ã€‚", 
        "embed_fail": "åŸ‹ã‚è¾¼ã¿å¤±æ•—: ãƒ•ãƒªãƒ¼ãƒ†ã‚£ã‚¢ã®ã‚¯ã‚©ãƒ¼ã‚¿è¶…éã¾ãŸã¯ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã®å•é¡Œã€‚",
        "gcs_not_conf": 'GCSãŒæœªè¨­å®šã‹ã€éŸ³å£°ãŒåˆ©ç”¨ã§ãã¾ã›ã‚“',
        "gcs_playback_fail": 'éŸ³å£°å†ç”Ÿã«å¤±æ•—ã—ã¾ã—ãŸ',
        "gcs_no_audio": 'éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ãªã— (GCSæœªè¨­å®š)',
        "transcribing": 'éŸ³å£°è»¢å†™ä¸­...',
        "playback": 'éŒ²éŸ³å†ç”Ÿ',
        "retranscribe": 'å†è»¢å†™',
        "error": 'ã‚¨ãƒ©ãƒ¼:',
        # ...
    }
}


# -----------------------------
# 6. Streamlit UI (ìŠ¤í¬ë¦½íŠ¸ì˜ ì²« ë²ˆì§¸ UI ì¶œë ¥ ëª…ë ¹ì–´)
# -----------------------------

# L ë³€ìˆ˜ ì ‘ê·¼ ì „ì— st.set_page_configë¥¼ ë¨¼ì € í˜¸ì¶œí•´ì•¼ í•¨.
# st.session_state.languageëŠ” DEFAULT_LANGìœ¼ë¡œ ì´ˆê¸°í™”ë˜ì—ˆë‹¤ê³  ê°€ì •í•˜ê³  Lì„ ì •ì˜.
L_pre = LANG[DEFAULT_LANG]

# â­â­â­ ì´ ì¤„ì´ Streamlit ìŠ¤í¬ë¦½íŠ¸ì˜ ì²« ë²ˆì§¸ ì‹¤í–‰ ëª…ë ¹ì–´ì—¬ì•¼ í•©ë‹ˆë‹¤. â­â­â­
st.set_page_config(page_title=L_pre["title"], layout="wide")

# ì´ì œ st.session_stateë¥¼ ì•ˆì „í•˜ê²Œ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
L = LANG[st.session_state.language] 
if 'language' not in st.session_state: st.session_state.language = DEFAULT_LANG

# -----------------------------
# 7. Core Initialization & Session State (í˜ì´ì§€ ì„¤ì • í›„ ì•ˆì „í•˜ê²Œ)
# -----------------------------

# --- Session State ì´ˆê¸°í™” (ë‚˜ë¨¸ì§€ ìƒíƒœ ë³€ìˆ˜) ---
if 'uploaded_files_state' not in st.session_state: st.session_state.uploaded_files_state = None
if 'is_llm_ready' not in st.session_state: st.session_state.is_llm_ready = False
if 'is_rag_ready' not in st.session_state: st.session_state.is_rag_ready = False
if 'firestore_db' not in st.session_state: st.session_state.firestore_db = None
if 'db_init_msg' not in st.session_state: st.session_state.db_init_msg = None
if 'gcs_init_msg' not in st.session_state: st.session_state.gcs_init_msg = None
if 'openai_init_msg' not in st.session_state: st.session_state.openai_init_msg = None
if 'llm_init_error_msg' not in st.session_state: st.session_state.llm_init_error_msg = None
if 'firestore_load_success' not in st.session_state: st.session_state.firestore_load_success = False
if "simulator_memory" not in st.session_state: st.session_state.simulator_memory = ConversationBufferMemory(memory_key="chat_history", return_messages=True)
if "simulator_messages" not in st.session_state: st.session_state.simulator_messages = []
if "initial_advice_provided" not in st.session_state: st.session_state.initial_advice_provided = False
if "simulator_chain" not in st.session_state: st.session_state.simulator_chain = None
if "is_chat_ended" not in st.session_state: st.session_state.is_chat_ended = False
if "show_delete_confirm" not in st.session_state: st.session_state.show_delete_confirm = False
if 'last_transcript' not in st.session_state: st.session_state['last_transcript'] = ''
if 'sim_audio_upload_key' not in st.session_state: st.session_state['sim_audio_upload_key'] = 0


# -----------------------------
# 8. Helper Functions (ìœ„ì¹˜ ì´ë™: ëª¨ë“  í—¬í¼ í•¨ìˆ˜ê°€ ì´ ì‹œì  ì´í›„ì— ì •ì˜ë˜ì–´ì•¼ í•¨)
# -----------------------------

def _load_service_account_from_secrets():
    """Secretsì—ì„œ ì„œë¹„ìŠ¤ ê³„ì • ì •ë³´ë¥¼ ì•ˆì „í•˜ê²Œ ë¡œë“œí•˜ê³  ë”•ì…”ë„ˆë¦¬ë¡œ ë°˜í™˜í•©ë‹ˆë‹¤. (UI ì¶œë ¥ ì—†ìŒ)"""
    if "FIREBASE_SERVICE_ACCOUNT_JSON" not in st.secrets:
        return None, "FIREBASE_SERVICE_ACCOUNT_JSON Secretì´ ëˆ„ë½ë˜ì—ˆìŠµë‹ˆë‹¤."
    service_account_data = st.secrets["FIREBASE_SERVICE_ACCOUNT_JSON"]
    sa_info = None
    if isinstance(service_account_data, str):
        try:
            sa_info = json.loads(service_account_data.strip())
        except json.JSONDecodeError as e:
            return None, f"FIREBASE_SERVICE_ACCOUNT_JSONì˜ JSON êµ¬ë¬¸ ì˜¤ë¥˜ì…ë‹ˆë‹¤. ìƒì„¸ ì˜¤ë¥˜: {e}"
    elif hasattr(service_account_data, 'get'):
        try:
            sa_info = dict(service_account_data)
        except Exception:
             return None, f"FIREBASE_SERVICE_ACCOUNT_JSONì˜ ë”•ì…”ë„ˆë¦¬ ë³€í™˜ ì‹¤íŒ¨."
    else:
        return None, f"FIREBASE_SERVICE_ACCOUNT_JSONì˜ í˜•ì‹ì´ ì˜¬ë°”ë¥´ì§€ ì•ŠìŠµë‹ˆë‹¤."
    
    if not sa_info.get("project_id") or not sa_info.get("private_key"):
        return None, "JSON ë‚´ 'project_id' ë˜ëŠ” 'private_key' í•„ë“œê°€ ëˆ„ë½ë˜ì—ˆìŠµë‹ˆë‹¤."
    return sa_info, None


@st.cache_resource(ttl=None)
def initialize_firestore_admin(L):
    """Secretsì—ì„œ ë¡œë“œëœ ì •ë³´ë¥¼ ì‚¬ìš©í•˜ì—¬ Firebase Admin SDKë¥¼ ì´ˆê¸°í™”í•˜ê³  DB í´ë¼ì´ì–¸íŠ¸ì™€ ì—ëŸ¬ ë©”ì‹œì§€ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤."""
    sa_info, error_message = _get_admin_credentials()
    if error_message:
        return None, f"âŒ Firebase Secret ì˜¤ë¥˜: {error_message}"
    
    db_client = None
    try:
        if firebase_admin._apps:
            db_client = firestore.client()
            return db_client, "âœ… Firestore DB í´ë¼ì´ì–¸íŠ¸ ì¤€ë¹„ ì™„ë£Œ"
        
        cred = credentials.Certificate(sa_info)
        initialize_app(cred)
        db_client = firestore.client()
        return db_client, "âœ… Firestore DB í´ë¼ì´ì–¸íŠ¸ ì¤€ë¹„ ì™„ë£Œ"
    except Exception as e:
        return None, f"ğŸ”¥ {L['firebase_init_fail']}: ì„œë¹„ìŠ¤ ê³„ì • ì •ë³´ ë¬¸ì œ. ì˜¤ë¥˜: {e}"


def get_gcs_bucket_name():
    return st.secrets.get('GCS_BUCKET_NAME') or os.environ.get('GCS_BUCKET_NAME')

@st.cache_resource
def init_gcs_client(L):
    """GCS í´ë¼ì´ì–¸íŠ¸ë¥¼ ì´ˆê¸°í™”í•˜ê³  í´ë¼ì´ì–¸íŠ¸ ê°ì²´ì™€ ì—ëŸ¬ ë©”ì‹œì§€ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤."""
    sa, _ = _load_service_account_from_secrets()
    gcs_bucket_name = get_gcs_bucket_name()
    
    if not gcs_bucket_name:
        return None, L['gcs_missing']
    if not sa:
        return None, "GCS ì´ˆê¸°í™”ë¥¼ ìœ„í•œ ì„œë¹„ìŠ¤ ê³„ì • ì •ë³´ ëˆ„ë½"
    
    gcs_client = None
    try:
        tmp = tempfile.NamedTemporaryFile(delete=False, suffix='.json')
        tmp.write(json.dumps(sa).encode('utf-8'))
        tmp.flush()
        os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = tmp.name
        gcs_client = storage.Client()
        return gcs_client, "âœ… GCS í´ë¼ì´ì–¸íŠ¸ ì¤€ë¹„ ì™„ë£Œ"
    except Exception as e:
        return None, f"{L['gcs_init_fail']}: {e}"


@st.cache_resource
def init_openai_client(L):
    """OpenAI í´ë¼ì´ì–¸íŠ¸ë¥¼ ì´ˆê¸°í™”í•˜ê³  í´ë¼ì´ì–¸íŠ¸ ê°ì²´ì™€ ì—ëŸ¬ ë©”ì‹œì§€ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤."""
    openai_key = st.secrets.get('OPENAI_API_KEY') or os.environ.get('OPENAI_API_KEY')
    if openai_key:
        try:
            return OpenAI(api_key=openai_key), "âœ… OpenAI í´ë¼ì´ì–¸íŠ¸ ì¤€ë¹„ ì™„ë£Œ"
        except Exception as e:
            return None, f"OpenAI client init error: {e}"
    return None, L['openai_missing']

# --- ë‚˜ë¨¸ì§€ Helper í•¨ìˆ˜ë“¤ (ìƒëµ) ---
# ... (upload_audio_to_gcs, download_audio_from_gcs, save_audio_record, 
# delete_audio_record, transcribe_bytes_with_whisper, save_simulation_history, 
# load_simulation_histories, delete_all_history, get_mock_response_data, 
# get_closing_messages, get_document_chunks, get_vector_store, get_rag_chain, 
# load_or_train_lstm, force_rerun_lstm, render_interactive_quiz, 
# synthesize_and_play_audio, render_tts_button, clean_and_load_json, etc.) ...

# Placeholders for necessary functions defined in section 2/3
def save_simulation_history(db, initial_query, customer_type, messages):
    L = LANG[st.session_state.language]
    if not db: st.sidebar.warning(L.get("firestore_no_db_connect")); return False
    history_data = [{k: v for k, v in msg.items()} for msg in messages]
    data = {"initial_query": initial_query, "customer_type": customer_type, "messages": history_data, "language_key": st.session_state.language, "timestamp": firestore.SERVER_TIMESTAMP}
    try: db.collection("simulation_histories").add(data); st.sidebar.success(L.get("save_history_success")); return True
    except Exception as e: st.sidebar.error(f"âŒ {L.get('save_history_fail')}: {e}"); return False

def load_simulation_histories(db): # Simplified
    if not db: return []; return []

def delete_all_history(db): # Simplified
    L = LANG[st.session_state.language]; st.success(L["delete_success"]); st.rerun()

def get_document_chunks(files): return []
def get_vector_store(text_chunks): return None
def get_rag_chain(vector_store): return None
def save_index_to_firestore(db, vector_store, index_id="user_portfolio_rag"): return True 
def load_index_from_firestore(db, embeddings, index_id="user_portfolio_rag"): return None 
def load_or_train_lstm(): return None, []
def render_interactive_quiz(quiz_data, current_lang): st.warning("Quiz UI Placeholder")
def synthesize_and_play_audio(current_lang_key): st.components.v1.html(f"""<script>window.speakText = (text, langKey) => {{ console.log('Speaking: ' + text + ' in ' + langKey); }}</script>""", height=5, width=0) 
def render_tts_button(text_to_speak, current_lang_key): st.button(LANG[current_lang_key].get("button_listen_audio"), key=f"tts_{hash(text_to_speak)}")
def clean_and_load_json(text): return None
def get_mock_response_data(lang_key, customer_type):
    L = LANG[lang_key]
    return {"advice_header": f"{L['simulation_advice_header']}", "advice": f"Mock advice for {customer_type}", "draft_header": f"{L['simulation_draft_header']}", "draft": f"Mock draft response in {lang_key}"}
def get_closing_messages(lang_key):
    if lang_key == 'ko': return {"additional_query": "ë˜ ë‹¤ë¥¸ ë¬¸ì˜ ì‚¬í•­ì€ ì—†ìœ¼ì‹ ê°€ìš”?", "chat_closing": LANG['ko']['prompt_survey']}
    elif lang_key == 'en': return {"additional_query": "Is there anything else we can assist you with today?", "chat_closing": LANG['en']['prompt_survey']}
    elif lang_key == 'ja': return {"additional_query": "ã¾ãŸã€ãŠå®¢æ§˜ã«ãŠæ‰‹ä¼ã„ã•ã›ã¦é ‚ã‘ã‚‹ãŠå•ã„åˆã‚ã›ã¯å¾¡åº§ã„ã¾ã›ã‚“ã‹ï¼Ÿ", "chat_closing": LANG['ja']['prompt_survey']}
    return get_closing_messages('ko')
# --- End Helper Functions ---


# --- í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì‹¤í–‰ ---
firestore_db_client, db_msg = initialize_firestore_admin(L)
st.session_state.firestore_db = firestore_db_client
st.session_state.db_init_msg = db_msg

gcs_client_obj, gcs_msg = init_gcs_client(L)
gcs_client = gcs_client_obj
st.session_state.gcs_init_msg = gcs_msg

openai_client_obj, openai_msg = init_openai_client(L)
openai_client = openai_client_obj
st.session_state.openai_init_msg = openai_msg

# --- LLM ì´ˆê¸°í™” ---
API_KEY = os.environ.get("GEMINI_API_KEY")
if 'llm' not in st.session_state and API_KEY:
    try:
        st.session_state.llm = ChatGoogleGenerativeAI(model="gemini-2.5-flash", temperature=0.7, google_api_key=API_KEY)
        st.session_state.embeddings = GoogleGenerativeAIEmbeddings(model="models/embedding-001", google_api_key=API_KEY)
        st.session_state.is_llm_ready = True
        
        SIMULATOR_PROMPT = PromptTemplate(
            template="The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context.\n\n{chat_history}\nHuman: {input}\nAI:",
            input_variables=["input", "chat_history"]
        )
        st.session_state.simulator_chain = ConversationChain(
            llm=st.session_state.llm,
            memory=st.session_state.simulator_memory,
            prompt=SIMULATOR_PROMPT,
            input_key="input",
        )
    except Exception as e:
        st.session_state.llm_init_error_msg = f"{L['llm_error_init']} (Gemini): {e}"
        st.session_state.is_llm_ready = False
elif not API_KEY:
    st.session_state.llm_init_error_msg = L["llm_error_key"]

# RAG Index Loading
if st.session_state.get('firestore_db') and 'conversation_chain' not in st.session_state and st.session_state.is_llm_ready:
    loaded_index = load_index_from_firestore(st.session_state.firestore_db, st.session_state.embeddings)
    if loaded_index:
        st.session_state.conversation_chain = get_rag_chain(loaded_index)
        st.session_state.is_rag_ready = True
        st.session_state.firestore_load_success = True
    else:
        st.session_state.firestore_load_success = False


# -----------------------------
# 9. UI RENDERING LOGIC
# -----------------------------

# ì‚¬ì´ë“œë°” ì„¤ì • ì‹œì‘
with st.sidebar:
    selected_lang_key = st.selectbox(
        L["lang_select"],
        options=['ko', 'en', 'ja'],
        index=['ko', 'en', 'ja'].index(st.session_state.language),
        format_func=lambda x: {"ko": "í•œêµ­ì–´", "en": "English", "ja": "æ—¥æœ¬èª"}[x],
    )
    
    if selected_lang_key != st.session_state.language:
        st.session_state.language = selected_lang_key
        st.rerun()
    
    L = LANG[st.session_state.language]
    st.title(L["sidebar_title"])
    
    st.markdown("---")
    st.subheader("í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ìƒíƒœ")
    
    # --- ì´ˆê¸°í™” ìƒíƒœ í‘œì‹œ ---
    if st.session_state.get('llm_init_error_msg'):
        st.error(st.session_state.llm_init_error_msg)
    elif st.session_state.is_llm_ready:
        st.success("âœ… LLM ë° ì„ë² ë”© í´ë¼ì´ì–¸íŠ¸ ì¤€ë¹„ ì™„ë£Œ")

    # DB & GCS ìƒíƒœ í‘œì‹œ
    if "âœ…" in st.session_state.db_init_msg: st.success(st.session_state.db_init_msg)
    else: st.warning(st.session_state.db_init_msg)
    
    if "âœ…" in st.session_state.gcs_init_msg: st.success(st.session_state.gcs_init_msg)
    else: st.warning(st.session_state.gcs_init_msg)
    
    if "âœ…" in st.session_state.openai_init_msg: st.success(st.session_state.openai_init_msg)
    else: st.warning(st.session_state.openai_init_msg)

    st.markdown("---")
    
    # RAG Indexing Section
    uploaded_files_widget = st.file_uploader(
        L["file_uploader"], type=["pdf","txt","html"], accept_multiple_files=True
    )
    if uploaded_files_widget: st.session_state.uploaded_files_state = uploaded_files_widget
    files_to_process = st.session_state.uploaded_files_state if st.session_state.uploaded_files_state else []
    
    if files_to_process and st.session_state.is_llm_ready and st.session_state.firestore_db:
        if st.button(L["button_start_analysis"], key="start_analysis"):
            with st.spinner(L["data_analysis_progress"]):
                text_chunks = get_document_chunks(files_to_process)
                vector_store = get_vector_store(text_chunks)
                if vector_store:
                    save_success = save_index_to_firestore(st.session_state.firestore_db, vector_store)
                    st.success(L["embed_success"].format(count=len(text_chunks)) + (" " + L["db_save_complete"] if save_success else " (DB Save Failed)"))
                    st.session_state.conversation_chain = get_rag_chain(vector_store)
                    st.session_state.is_rag_ready = True
                else:
                    st.session_state.is_rag_ready = False
                    st.error(L["embed_fail"])
    elif not files_to_process:
        st.warning(L.get("warning_no_files")) 

    st.markdown("---")
    
    # Feature Selection Radio
    feature_selection = st.radio(
        "ê¸°ëŠ¥ ì„ íƒ", 
        [L["rag_tab"], L["content_tab"], L["lstm_tab"], L["simulator_tab"], L["voice_rec_header"]]
    )

st.title(L["title"])

# ================================
# 10. ê¸°ëŠ¥ë³„ í˜ì´ì§€ êµ¬í˜„
# ================================

if feature_selection == L["voice_rec_header"]:
    st.header(L['voice_rec_header'])
    st.caption(L['record_help'])

    col_rec_ui, col_list_ui = st.columns([1, 1])

    with col_rec_ui:
        st.subheader(L['rec_header'])
        
        # Audio Input Widget
        audio_obj = None
        try:
            if hasattr(st, 'audio_input'):
                audio_obj = st.audio_input(L["button_mic_input"], key='main_recorder_input')
        except Exception:
            audio_obj = None

        if audio_obj is None:
            st.caption(f"({L['uploaded_file']}ë¡œ ëŒ€ì²´)")
            audio_obj = st.file_uploader(L['uploaded_file'], type=['wav', 'mp3', 'm4a', 'webm'], key='main_file_uploader')

        audio_bytes = None
        audio_mime = 'audio/webm'
        if audio_obj is not None:
            if hasattr(audio_obj, 'getvalue'):
                audio_bytes = audio_obj.getvalue()
                audio_mime = getattr(audio_obj, 'type', 'audio/webm')
        
        if audio_bytes:
            st.audio(audio_bytes, format=audio_mime)
            
            # Transcribe Action
            if st.button(L['transcribe_btn'], key='transcribe_btn_key_rec'):
                if openai_client is None:
                    st.error(L['openai_missing'])
                else:
                    with st.spinner(L['transcribing']):
                        try:
                            transcript_text = transcribe_bytes_with_whisper(audio_bytes, audio_mime)
                            st.session_state['last_transcript'] = transcript_text
                            st.success(L['transcript_result'])
                        except RuntimeError as e:
                            st.error(e)

            st.text_area(L['transcript_text'], value=st.session_state.get('last_transcript', ''), height=150, key='transcript_area_rec')

            # Save Action
            if st.button(L['save_btn'], key='save_btn_key_rec'):
                if st.session_state.firestore_db is None:
                    st.error(L['firebase_init_fail'])
                else:
                    bucket_name = get_gcs_bucket_name()
                    ext = audio_mime.split('/')[-1] if '/' in audio_mime else 'webm'
                    filename = f"record_{int(time.time())}.{ext}"
                    transcript_text = st.session_state.get('last_transcript', '')
                    
                    try:
                        save_audio_record(st.session_state.firestore_db, bucket_name, audio_bytes, filename, transcript_text, mime_type=audio_mime)
                        st.success(L['saved_success'])
                        st.session_state['last_transcript'] = ''
                        st.experimental_rerun()
                    except Exception as e:
                        st.error(f"{L['error']} {e}")

    with col_list_ui:
        st.subheader(L['rec_list_title'])
        if st.session_state.firestore_db is None:
            st.warning(L['firebase_init_fail'] + ' â€” ì´ë ¥ ê¸°ëŠ¥ ì‚¬ìš© ë¶ˆê°€')
        else:
            try:
                docs = list(st.session_state.firestore_db.collection('voice_records').order_by('created_at', direction=firestore.Query.DESCENDING).limit(50).stream())
            except Exception as e:
                st.error(f"Firestore read error: {e}")
                docs = []

            if not docs:
                st.info(L['no_records'])
            else:
                bucket_name = get_gcs_bucket_name()
                for d in docs:
                    data = d.to_dict()
                    doc_id = d.id
                    created_str = data.get('created_at').astimezone(timezone.utc).strftime('%Y-%m-%d %H:%M UTC') if isinstance(data.get('created_at'), datetime) else str(data.get('created_at'))
                    transcript_snippet = (data.get('transcript') or '')[:50].replace('\n', ' ') + '...'

                    with st.expander(f"[{created_str}] {transcript_snippet}"):
                        st.write(f"**{L['transcript_text']}:** {data.get('transcript') or 'N/A'}")
                        st.caption(f"**Size:** {data.get('size')} bytes | **Path:** {data.get('gcs_path', L['gcs_not_conf'])}")

                        colp, colr, cold = st.columns([2, 1, 1])
                        
                        # Playback Button
                        if colp.button(L['playback'], key=f'play_{doc_id}'):
                            if data.get('gcs_path') and gcs_client and bucket_name:
                                with st.spinner(L['playback']):
                                    try:
                                        blob_bytes = download_audio_from_gcs(bucket_name, data['gcs_path'].split(f'gs://{bucket_name}/')[-1])
                                        mime_type = data.get('mime_type', 'audio/webm')
                                        st.audio(blob_bytes, format=mime_type)
                                    except Exception as e:
                                        st.error(f"{L['gcs_playback_fail']}: {e}")
                            else:
                                st.info(L['gcs_no_audio'])

                        # Re-transcribe Button
                        if colr.button(L['retranscribe'], key=f'retx_{doc_id}'):
                            if openai_client is None: st.error(L['openai_missing'])
                            elif data.get('gcs_path') and gcs_client and bucket_name:
                                with st.spinner(L['transcribing']):
                                    try:
                                        blob_bytes = download_audio_from_gcs(bucket_name, data['gcs_path'].split(f'gs://{bucket_name}/')[-1])
                                        mime_type = data.get('mime_type', 'audio/webm')
                                        new_text = transcribe_bytes_with_whisper(blob_bytes, mime_type)
                                        st.session_state.firestore_db.collection('voice_records').document(doc_id).update({'transcript': new_text})
                                        st.success(L['retranscribe'] + ' ' + L['saved_success'])
                                        st.experimental_rerun()
                                    except Exception as e:
                                        st.error(f"{L['error']} {e}")
                            else: st.error(L['gcs_not_conf'])

                        # Delete Button
                        if cold.button(L['delete'], key=f'del_{doc_id}'):
                            if st.session_state.get(f'confirm_del_rec_{doc_id}', False):
                                ok = delete_audio_record(st.session_state.firestore_db, bucket_name, doc_id)
                                if ok: st.success(L['delete_success'])
                                else: st.error(L['delete_fail'])
                                st.session_state[f'confirm_del_rec_{doc_id}'] = False
                                st.experimental_rerun()
                            else:
                                st.session_state[f'confirm_del_rec_{doc_id}'] = True
                                st.warning(L['delete_confirm_rec'])

elif feature_selection == L["simulator_tab"]: 
    # (Simulator UI logic remains the same, using st.session_state.firestore_db)
    st.header(L["simulator_header"])
    st.markdown(L["simulator_desc"])
    
    # 1. TTS ìœ í‹¸ë¦¬í‹° (ìƒíƒœ í‘œì‹œê¸° ë° JS í•¨ìˆ˜)ë¥¼ í˜ì´ì§€ ìƒë‹¨ì— ì‚½ì…
    st.markdown(f'<div id="tts_status" style="padding: 5px; text-align: center; border-radius: 5px; background-color: #f0f0f0; margin-bottom: 10px;">{L["tts_status_ready"]}</div>', unsafe_allow_html=True)
    if "tts_js_loaded" not in st.session_state:
         synthesize_and_play_audio(st.session_state.language) 
         st.session_state.tts_js_loaded = True

    # 1.5 ì´ë ¥ ì‚­ì œ ë²„íŠ¼ ë° ëª¨ë‹¬
    db = st.session_state.get('firestore_db')
    col_delete, _ = st.columns([1, 4])
    with col_delete:
        if st.button(L["delete_history_button"], key="trigger_delete_history_sim"):
            st.session_state.show_delete_confirm = True

    if st.session_state.show_delete_confirm:
        with st.container(border=True):
            st.warning(L["delete_confirm_message"])
            col_yes, col_no = st.columns(2)
            if col_yes.button(L["delete_confirm_yes"], key="confirm_delete_yes", type="primary"):
                with st.spinner(L["deleting_history_progress"]): 
                    delete_all_history(db)
            if col_no.button(L["delete_confirm_no"], key="confirm_delete_no"):
                st.session_state.show_delete_confirm = False
                st.rerun()

    # â­ Firebase ìƒë‹´ ì´ë ¥ ë¡œë“œ ë° ì„ íƒ ì„¹ì…˜
    if db:
        with st.expander(L["history_expander_title"]):
            histories = load_simulation_histories(db)
            search_query = st.text_input(L["search_history_label"], key="history_search_sim", value="")
            today = datetime.now().date()
            default_start_date = today - timedelta(days=7)
            date_range_input = st.date_input(L["date_range_label"], value=[default_start_date, today], key="history_date_range_sim")

            filtered_histories = []
            if histories:
                if isinstance(date_range_input, list) and len(date_range_input) == 2:
                    start_date = min(date_range_input)
                    end_date = max(date_range_input) + timedelta(days=1)
                else:
                    start_date = datetime.min.date()
                    end_date = datetime.max.date()
                for h in histories:
                    search_match = True
                    if search_query:
                        query_lower = search_query.lower()
                        searchable_text = h['initial_query'].lower() + " " + h['customer_type'].lower()
                        if query_lower not in searchable_text: search_match = False
                    date_match = True
                    if h.get('timestamp'):
                        h_date = h['timestamp'].date()
                        if not (start_date <= h_date < end_date): date_match = False
                    if search_match and date_match: filtered_histories.append(h)
            
            if filtered_histories:
                history_options = {f"[{h['timestamp'].strftime('%m-%d %H:%M')}] {h['customer_type']} - {h['initial_query'][:30]}...": h for h in filtered_histories}
                selected_key = st.selectbox(L["history_selectbox_label"], options=list(history_options.keys()))
                
                if st.button(L["history_load_button"], key='load_sim_history'): 
                    selected_history = history_options[selected_key]
                    st.session_state.customer_query_text_area = selected_history['initial_query']
                    st.session_state.initial_advice_provided = True
                    st.session_state.simulator_messages = selected_history['messages']
                    st.session_state.is_chat_ended = selected_history.get('is_chat_ended', False)
                    st.session_state.simulator_memory.clear()
                    for msg in selected_history['messages']:
                         if msg['role'] == 'customer' or msg['role'] == 'agent_response': st.session_state.simulator_memory.chat_memory.add_user_message(msg['content'])
                         elif msg['role'] in ['supervisor', 'customer_rebuttal', 'customer_end', 'system_end']: st.session_state.simulator_memory.chat_memory.add_ai_message(msg['content'])
                    st.rerun()
            else:
                 st.info(L.get("no_history_found"))

    # LLM and UI logic for Simulation flow
    if st.session_state.is_llm_ready or not os.environ.get("GEMINI_API_KEY"):
        if st.session_state.is_chat_ended:
            st.success(L["prompt_customer_end"] + " " + L["prompt_survey"])
            if st.button(L["new_simulation_button"], key="new_simulation"): 
                 st.session_state.is_chat_ended = False
                 st.session_state.initial_advice_provided = False
                 st.session_state.simulator_messages = []
                 st.session_state.simulator_memory.clear()
                 st.session_state['last_transcript'] = ''
                 st.rerun()
            st.stop()
        
        if 'customer_query_text_area' not in st.session_state: st.session_state.customer_query_text_area = ""

        customer_query = st.text_area(
            L["customer_query_label"], key="customer_query_text_area", height=150, placeholder=L["initial_query_sample"], 
            disabled=st.session_state.initial_advice_provided
        )
        customer_type_options_list = L["customer_type_options"]
        default_index = 1 if len(customer_type_options_list) > 1 else 0
        customer_type_display = st.selectbox(
            L["customer_type_label"], customer_type_options_list, index=default_index, disabled=st.session_state.initial_advice_provided
        )
        current_lang_key = st.session_state.language 

        if st.button(L["button_simulate"], key="start_simulation", disabled=st.session_state.initial_advice_provided):
            if not customer_query: st.warning(L["simulation_warning_query"]); st.stop()
            
            st.session_state.simulator_memory.clear()
            st.session_state.simulator_messages = []
            st.session_state.is_chat_ended = False
            st.session_state.simulator_messages.append({"role": "customer", "content": customer_query})
            st.session_state.simulator_memory.chat_memory.add_user_message(customer_query)
            
            initial_prompt = f"""You are an AI Customer Support Supervisor... [CRITICAL RULE FOR DRAFT CONTENT]... When the Agent subsequently asks for information, **Roleplay as the Customer** who is frustrated but **MUST BE HIGHLY COOPERATIVE** and provide the requested details piece by piece (not all at once). The customer MUST NOT argue or ask why the information is needed... The recommended draft MUST be strictly in {LANG[current_lang_key]['lang_select']}."""

            if not os.environ.get("GEMINI_API_KEY"):
                mock_data = get_mock_response_data(current_lang_key, customer_type_display)
                ai_advice_text = f"### {mock_data['advice_header']}\n\n{mock_data['advice']}\n\n### {mock_data['draft_header']}\n\n{mock_data['draft']}"
                st.session_state.simulator_messages.append({"role": "supervisor", "content": ai_advice_text})
                st.session_state.simulator_memory.chat_memory.add_ai_message(ai_advice_text)
                st.session_state.initial_advice_provided = True
                save_simulation_history(db, customer_query, customer_type_display, st.session_state.simulator_messages)
                st.rerun() 
            
            if os.environ.get("GEMINI_API_KEY"):
                with st.spinner(L["response_generating"]):
                    try:
                        response_text = st.session_state.simulator_chain.predict(input=initial_prompt)
                        st.session_state.simulator_messages.append({"role": "supervisor", "content": response_text})
                        st.session_state.initial_advice_provided = True
                        save_simulation_history(db, customer_query, customer_type_display, st.session_state.simulator_messages)
                        st.rerun() 
                    except Exception as e:
                        st.error(f"AI ì¡°ì–¸ ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        
        st.markdown("---")
        for message in st.session_state.simulator_messages:
            if message["role"] == "customer": with st.chat_message("user", avatar="ğŸ™‹"): st.markdown(message["content"])
            elif message["role"] == "supervisor": with st.chat_message("assistant", avatar="ğŸ¤–"): st.markdown(message["content"]); render_tts_button(message["content"], st.session_state.language) 
            elif message["role"] == "agent_response": with st.chat_message("user", avatar="ğŸ§‘â€ğŸ’»"): st.markdown(message["content"])
            elif message["role"] == "customer_rebuttal": with st.chat_message("assistant", avatar="ğŸ˜ "): st.markdown(message["content"])
            elif message["role"] == "customer_end": with st.chat_message("assistant", avatar="ğŸ˜Š"): st.markdown(message["content"])
            elif message["role"] == "system_end": with st.chat_message("assistant", avatar="âœ¨"): st.markdown(message["content"])

        if st.session_state.initial_advice_provided and not st.session_state.is_chat_ended:
            last_role = st.session_state.simulator_messages[-1]['role'] if st.session_state.simulator_messages else None
            
            if last_role in ["customer_rebuttal", "customer_end", "supervisor", "customer"]:
                st.markdown(f"### {L['agent_response_header']}") 
                
                col_audio, col_text_area = st.columns([1, 2])
                
                # --- Whisper Audio Input for Agent Response ---
                with col_audio:
                    audio_file = st.audio_input(L["button_mic_input"], key=f"sim_audio_input_{st.session_state['sim_audio_upload_key']}")
                
                if audio_file:
                    if openai_client is None: st.error(L.get("whisper_client_error"))
                    else:
                        with st.spinner(L.get("whisper_processing")):
                            try:
                                mime_type = getattr(audio_file, 'type', 'audio/webm')
                                transcribed_text = transcribe_bytes_with_whisper(audio_file.getvalue(), mime_type)
                                st.session_state['last_transcript'] = transcribed_text
                                st.session_state['sim_audio_upload_key'] += 1
                                st.success(L.get("whisper_success"))
                                st.rerun() 
                            except Exception as e: st.error(f"ìŒì„± ì „ì‚¬ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}"); st.session_state['last_transcript'] = ""

                agent_response = col_text_area.text_area(
                    L["agent_response_placeholder"], value=st.session_state['last_transcript'], key="agent_response_area_text", height=150
                )
                
                # JS Enter Key Listener
                st.components.v1.html("""<script>const textarea = document.querySelector('textarea[key="agent_response_area_text"]'); const button = document.querySelector('button[key="send_agent_response_sim"]'); if (textarea && button) { textarea.addEventListener('keydown', function(event) { if (event.key === 'Enter' && (!event.shiftKey && !event.ctrlKey)) { event.preventDefault(); button.click(); } }); }</script>""", height=0, width=0)

                if st.button(L["send_response_button"], key="send_agent_response_sim"): 
                    if agent_response.strip():
                        st.session_state['last_transcript'] = ""
                        st.session_state.simulator_messages.append({"role": "agent_response", "content": agent_response})
                        st.session_state.simulator_memory.chat_memory.add_user_message(agent_response)
                        save_simulation_history(db, st.session_state.customer_query_text_area, customer_type_display, st.session_state.simulator_messages)
                        st.rerun()
                    else: st.warning(L.get("empty_response_warning"))
            
            if last_role == "agent_response":
                col_end, col_next = st.columns([1, 2])
                
                if col_end.button(L["button_end_chat"], key="end_chat_sim"): 
                    closing_messages = get_closing_messages(current_lang_key)
                    st.session_state.simulator_messages.append({"role": "supervisor", "content": closing_messages["additional_query"]})
                    st.session_state.simulator_memory.chat_memory.add_ai_message(closing_messages["additional_query"])
                    st.session_state.simulator_messages.append({"role": "system_end", "content": closing_messages["chat_closing"]})
                    st.session_state.simulator_memory.chat_memory.add_ai_message(closing_messages["chat_closing"])
                    st.session_state.is_chat_ended = True
                    save_simulation_history(db, st.session_state.customer_query_text_area, customer_type_display, st.session_state.simulator_messages)
                    st.rerun()

                if col_next.button(L["request_rebuttal_button"], key="request_rebuttal_sim"):
                    if not os.environ.get("GEMINI_API_KEY"): st.warning("API Keyê°€ ì—†ì–´ LLM ì‹œë®¬ë ˆì´ì…˜ ë¶ˆê°€"); st.stop()
                    
                    next_reaction_prompt = f"""Analyze the entire chat history. Roleplay as the customer ({customer_type_display}). Based on the agent's last message, generate ONE of the following responses... The response MUST be strictly in {LANG[current_lang_key]['lang_select']}."""
                    
                    with st.spinner(L["response_generating"]):
                        try:
                            customer_reaction = st.session_state.simulator_chain.predict(input=next_reaction_prompt)
                            positive_keywords = ["ê°ì‚¬", "thank you", "ã‚ã‚ŠãŒã¨ã†", L['customer_positive_response'].lower().split('/')[-1].strip()]
                            is_positive_close = any(keyword in customer_reaction.lower() for keyword in positive_keywords)
                            
                            if is_positive_close:
                                role = "customer_end"
                                st.session_state.simulator_messages.append({"role": role, "content": customer_reaction})
                                st.session_state.simulator_memory.chat_memory.add_ai_message(customer_reaction)
                                st.session_state.simulator_messages.append({"role": "supervisor", "content": L["customer_closing_confirm"]})
                                st.session_state.simulator_memory.chat_memory.add_ai_message(L["customer_closing_confirm"])
                            else:
                                role = "customer_rebuttal"
                                st.session_state.simulator_messages.append({"role": role, "content": customer_reaction})
                                st.session_state.simulator_memory.chat_memory.add_ai_message(customer_reaction)
                                 
                            save_simulation_history(db, st.session_state.customer_query_text_area, customer_type_display, st.session_state.simulator_messages)
                            st.rerun()
                        except Exception as e: st.error(f"LLM ì‘ë‹µ ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
    else:
        st.error(L["llm_error_init"])

elif feature_selection == L["rag_tab"]:
    # (RAG Chatbot UI logic remains the same)
    st.header(L["rag_header"])
    st.markdown(L["rag_desc"])
    if st.session_state.get('is_rag_ready', False) and st.session_state.get('conversation_chain'):
        if "messages" not in st.session_state: st.session_state.messages = []
        for message in st.session_state.messages:
            with st.chat_message(message["role"]): st.markdown(message["content"])
        if prompt := st.chat_input(L["rag_input_placeholder"]):
            st.session_state.messages.append({"role":"user","content":prompt})
            with st.chat_message("user"): st.markdown(prompt)
            with st.chat_message("assistant"):
                with st.spinner(L["response_generating"]):
                    try:
                        response = st.session_state.conversation_chain.invoke({"question":prompt})
                        answer = response.get('answer', 'ì‘ë‹µì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.' if st.session_state.language == 'ko' else 'Could not generate response.')
                        st.markdown(answer)
                        st.session_state.messages.append({"role":"assistant","content":answer})
                    except Exception as e: st.error(f"ì±—ë´‡ ì˜¤ë¥˜: {e}"); st.session_state.messages.append({"role":"assistant","content":"ì˜¤ë¥˜ ë°œìƒ" if st.session_state.language == 'ko' else "An error occurred"})
    else: st.warning(L["warning_rag_not_ready"])

elif feature_selection == L["content_tab"]:
    # (Custom Content Generation UI logic remains the same)
    st.header(L["content_header"])
    st.markdown(L["content_desc"])
    if st.session_state.is_llm_ready:
        topic = st.text_input(L["topic_label"])
        level_map = dict(zip(L["level_options"], ["Beginner", "Intermediate", "Advanced"]))
        content_map = dict(zip(L["content_options"], ["summary", "quiz", "example"]))
        level_display = st.selectbox(L["level_label"], L["level_options"])
        content_type_display = st.selectbox(L["content_type_label"], L["content_options"])
        level = level_map[level_display]
        content_type = content_map[content_type_display]

        if st.button(L["button_generate"]):
            if topic:
                target_lang = {"ko": "Korean", "en": "English", "ja": "Japanese"}[st.session_state.language]
                if content_type == 'quiz':
                    full_prompt = f"""You are a professional AI coach at the {level} level. Please generate exactly 10 multiple-choice questions about the topic in {target_lang}. Your entire response MUST be a valid JSON object wrapped in
                else:
                    display_type_text = L["content_options"][L["content_options"].index(content_type_display)]
                    full_prompt = f"""You are a professional AI coach at the {level} level. Please generate clear and educational content in the requested {display_type_text} format based on the topic. The response MUST be strictly in {target_lang}. Topic: {topic}. Requested Format: {display_type_text}"""
                
                with st.spinner(f"Generating {content_type_display} for {topic}..."):
                    quiz_data_raw = None
                    try:
                        response = st.session_state.llm.invoke(full_prompt)
                        quiz_data_raw = response.content
                        st.session_state.quiz_data_raw = quiz_data_raw
                        if content_type == 'quiz':
                            quiz_data = clean_and_load_json(quiz_data_raw)
                            if quiz_data and 'quiz_questions' in quiz_data:
                                st.session_state.quiz_data = quiz_data
                                st.session_state.current_question = 0
                                st.session_state.quiz_submitted = False
                                st.session_state.quiz_results = [None] * len(quiz_data.get('quiz_questions',[]))
                                st.success(f"**{topic}** - **{content_type_display}** Result:")
                            else: st.error(L["quiz_error_llm"]); st.markdown(f"**{L['quiz_original_response']}**:"); st.code(quiz_data_raw, language="json")
                        else: st.success(f"**{topic}** - **{content_type_display}** Result:"); st.markdown(response.content)
                    except Exception as e: st.error(f"Content Generation Error: {e}"); 
            else: st.warning(L["warning_topic"])
    else: st.error(L["llm_error_init"])
    is_quiz_ready = content_type == 'quiz' and 'quiz_data' in st.session_state and st.session_state.quiz_data
    if is_quiz_ready and st.session_state.get('current_question', 0) < len(st.session_state.quiz_data.get('quiz_questions', [])):
        render_interactive_quiz(st.session_state.quiz_data, st.session_state.language)

elif feature_selection == L["lstm_tab"]:
    # (LSTM UI logic remains the same)
    st.header(L["lstm_header"])
    st.markdown(L["lstm_desc"])
    if st.button(L["lstm_rerun_button"], key="rerun_lstm", on_click=force_rerun_lstm): pass
    try:
        model, data = load_or_train_lstm()
        look_back = 5
        X_input = np.reshape(data[-look_back:], (1, look_back, 1))
        predicted_score = model.predict(X_input, verbose=0)[0][0]
        st.markdown("---")
        st.subheader(L["lstm_result_header"])
        col_score, col_chart = st.columns([1, 2])
        with col_score:
            st.metric(L["lstm_score_metric"], f"{predicted_score:.1f}{'ì ' if st.session_state.language == 'ko' else ''}")
            st.info(L["lstm_score_info"].format(predicted_score=predicted_score))
        with col_chart:
            fig, ax = plt.subplots(figsize=(8, 4))
            ax.plot(data, label='Past Scores', marker='o')
            ax.plot(len(data), predicted_score, label='Predicted Next Score', marker='*', color='red', markersize=10)
            ax.set_title(L["lstm_header"])
            ax.set_xlabel(f"Time ({L.get('score', 'Score')} attempts)")
            ax.set_ylabel(f"{L.get('score', 'Score')} (0-100)")
            ax.legend()
            st.pyplot(fig)
    except Exception as e:
        st.error(f"LSTM ëª¨ë¸ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. (ì˜¤ë¥˜ ë©”ì‹œì§€: {e})")  
